{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhigoel25/AV_TrafficDetectionModel/blob/main/AV_TrafficSignDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu_3lPGPHY1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from PIL import Image\n",
        "drive.mount('/content/drive')\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw9JdFKE3zVM"
      },
      "outputs": [],
      "source": [
        "trainingImage_folder_path = '/content/drive/MyDrive/Stanford_Dataset/Images/Train_Images'\n",
        "testingImage_folder_path = '/content/drive/MyDrive/Stanford_Dataset/Images/Test_Images'\n",
        "testingLabel_folder_path = '/content/drive/MyDrive/Stanford_Dataset/Labels/Test_Labels'\n",
        "trainingLabel_folder_path = '/content/drive/MyDrive/Stanford_Dataset/Labels/Train_Labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZh-EZVtOs0-"
      },
      "outputs": [],
      "source": [
        "class YoloDataset(Dataset):\n",
        "  def __init__(self, images_dir, annotations_dir, transform=None):\n",
        "    self.images_dir = images_dir\n",
        "    self.annotations_dir = annotations_dir\n",
        "    self.transform = transform\n",
        "    self.image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_file = self.image_files[idx]\n",
        "      image_path = os.path.join(self.images_dir, image_file)\n",
        "      if image_file.endswith('.jpg'):\n",
        "         annotation_path = os.path.join(self.annotations_dir, image_file.replace('.jpg', '.txt'))\n",
        "      elif image_file.endswith('.png'):\n",
        "         annotation_path = os.path.join(self.annotations_dir, image_file.replace('.png', '.txt'))\n",
        "\n",
        "      image = Image.open(image_path).convert(\"RGB\")\n",
        "      boxes = []\n",
        "      labels = []\n",
        "\n",
        "      if os.path.exists(annotation_path):\n",
        "        with open(annotation_path, 'r') as file:\n",
        "          for line in file:\n",
        "            parts = line.strip().split()\n",
        "            label = int(parts[0]) + 1\n",
        "            bbox = list(map(float, parts[1:]))\n",
        "            bbox = yolo_to_rcnn(bbox, image.width, image.height)\n",
        "            boxes.append(bbox)\n",
        "            labels.append(label)\n",
        "      else:\n",
        "        print(f\"Annotation file missing for image: {image_file}\")\n",
        "        return None, None\n",
        "\n",
        "      boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "      labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      target = {}\n",
        "      target['boxes'] = boxes\n",
        "      target['labels'] = labels\n",
        "\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "      if boxes.numel() == 0:  # Skip if no boxes\n",
        "        return None, None\n",
        "\n",
        "      return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None and x[1] is not None, batch))  # Filter out None values\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "    images, targets = zip(*batch)\n",
        "    max_width = 640\n",
        "    max_height = 360\n",
        "\n",
        "    # Resize images\n",
        "    resized_images = []\n",
        "    adjusted_targets = []\n",
        "    for image, target in zip(images, targets):\n",
        "        # Resize image\n",
        "        resized_image = resize_image(image, (max_height, max_width))\n",
        "\n",
        "        # Adjust bounding boxes\n",
        "        original_width, original_height = image.shape[2], image.shape[1]\n",
        "        width_scale = max_width / original_width\n",
        "        height_scale = max_height / original_height\n",
        "\n",
        "        adjusted_boxes = []\n",
        "        for bbox in target['boxes']:\n",
        "            x_min, y_min, x_max, y_max = bbox.tolist()\n",
        "            adjusted_bbox = [\n",
        "                x_min * width_scale,\n",
        "                y_min * height_scale,\n",
        "                x_max * width_scale,\n",
        "                y_max * height_scale\n",
        "            ]\n",
        "            adjusted_boxes.append(adjusted_bbox)\n",
        "\n",
        "        adjusted_target = {\n",
        "            'boxes': torch.tensor(adjusted_boxes, dtype=torch.float32),\n",
        "            'labels': target['labels']\n",
        "        }\n",
        "\n",
        "        resized_images.append(resized_image)\n",
        "        adjusted_targets.append(adjusted_target)\n",
        "\n",
        "    images = torch.stack(resized_images, dim=0)\n",
        "    return images, adjusted_targets\n",
        "\n",
        "def resize_image(image, target_size):\n",
        "    return F.resize(image, target_size)\n",
        "\n",
        "def yolo_to_rcnn(bbox, image_width, image_height):\n",
        "    #print(\"Image Width {:.2f}\".format(image_width))\n",
        "    #print(\"Image Height {:.2f}\".format(image_height))\n",
        "\n",
        "    x_center, y_center, width, height = bbox\n",
        "    x_min = (x_center - width / 2) * image_width\n",
        "    x_max = (x_center + width / 2) * image_width\n",
        "    y_min = (y_center - height / 2) * image_height\n",
        "    y_max = (y_center + height / 2) * image_height\n",
        "\n",
        "    #print(\"x-center {:.2f}\".format(x_center))\n",
        "    #print(\"y_center {:.2f}\".format(y_center))\n",
        "    #print(\"width of box {:.2f}\".format(width))\n",
        "    #print(\"height of box {:.2f}\".format(height))\n",
        "    #print(\"x_min {:.2f}\".format(x_min))\n",
        "    #print(\"y_min {:.2f}\".format(y_min))\n",
        "    #print(\"x_max {:.2f}\".format(y_max))\n",
        "    #print(\"y_max {:.2f}\".format(y_max))\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "images_dir = trainingImage_folder_path\n",
        "annotations_dir = trainingLabel_folder_path\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "dataset = YoloDataset(images_dir, annotations_dir, transform)\n",
        "dataloader = DataLoader(dataset, batch_size =4, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YZxmJ7vMR--"
      },
      "outputs": [],
      "source": [
        "class TestingDataset(Dataset):\n",
        "  def __init__(self, images_dir, annotations_dir, transform=None):\n",
        "    self.images_dir = images_dir\n",
        "    self.annotations_dir = annotations_dir\n",
        "    self.transform = transform\n",
        "    self.image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_file = self.image_files[idx]\n",
        "      image_path = os.path.join(self.images_dir, image_file)\n",
        "      if image_file.endswith('.jpg'):\n",
        "         annotation_path = os.path.join(self.annotations_dir, image_file.replace('.jpg', '.txt'))\n",
        "      elif image_file.endswith('.png'):\n",
        "         annotation_path = os.path.join(self.annotations_dir, image_file.replace('.png', '.txt'))\n",
        "\n",
        "      image = Image.open(image_path).convert(\"RGB\")\n",
        "      boxes = []\n",
        "      labels = []\n",
        "\n",
        "      if os.path.exists(annotation_path):\n",
        "        with open(annotation_path, 'r') as file:\n",
        "          for line in file:\n",
        "            parts = line.strip().split()\n",
        "            label = int(parts[0]) + 1\n",
        "            bbox = list(map(float, parts[1:]))\n",
        "            bbox = yolo_to_rcnn(bbox, image.width, image.height)\n",
        "            boxes.append(bbox)\n",
        "            labels.append(label)\n",
        "      else:\n",
        "        print(f\"Annotation file missing for image: {image_file}\")\n",
        "        return None, None\n",
        "\n",
        "      boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "      labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      target = {}\n",
        "      target['boxes'] = boxes\n",
        "      target['labels'] = labels\n",
        "\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "      if boxes.numel() == 0:  # Skip if no boxes\n",
        "        return None, None\n",
        "\n",
        "      return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None and x[1] is not None, batch))  # Filter out None values\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "    images, targets = zip(*batch)\n",
        "    max_width = 640\n",
        "    max_height = 360\n",
        "\n",
        "    # Resize images\n",
        "    resized_images = []\n",
        "    adjusted_targets = []\n",
        "    for image, target in zip(images, targets):\n",
        "        # Resize image\n",
        "        resized_image = resize_image(image, (max_height, max_width))\n",
        "\n",
        "        # Adjust bounding boxes\n",
        "        original_width, original_height = image.shape[2], image.shape[1]\n",
        "        width_scale = max_width / original_width\n",
        "        height_scale = max_height / original_height\n",
        "\n",
        "        adjusted_boxes = []\n",
        "        for bbox in target['boxes']:\n",
        "            x_min, y_min, x_max, y_max = bbox.tolist()\n",
        "            adjusted_bbox = [\n",
        "                x_min * width_scale,\n",
        "                y_min * height_scale,\n",
        "                x_max * width_scale,\n",
        "                y_max * height_scale\n",
        "            ]\n",
        "            adjusted_boxes.append(adjusted_bbox)\n",
        "\n",
        "        adjusted_target = {\n",
        "            'boxes': torch.tensor(adjusted_boxes, dtype=torch.float32),\n",
        "            'labels': target['labels']\n",
        "        }\n",
        "\n",
        "        resized_images.append(resized_image)\n",
        "        adjusted_targets.append(adjusted_target)\n",
        "\n",
        "    images = torch.stack(resized_images, dim=0)\n",
        "    return images, adjusted_targets\n",
        "\n",
        "def resize_image(image, target_size):\n",
        "    return F.resize(image, target_size)\n",
        "\n",
        "def yolo_to_rcnn(bbox, image_width, image_height):\n",
        "    #print(\"Image Width {:.2f}\".format(image_width))\n",
        "    #print(\"Image Height {:.2f}\".format(image_height))\n",
        "\n",
        "    x_center, y_center, width, height = bbox\n",
        "    x_min = (x_center - width / 2) * image_width\n",
        "    x_max = (x_center + width / 2) * image_width\n",
        "    y_min = (y_center - height / 2) * image_height\n",
        "    y_max = (y_center + height / 2) * image_height\n",
        "\n",
        "    #print(\"x-center {:.2f}\".format(x_center))\n",
        "    #print(\"y_center {:.2f}\".format(y_center))\n",
        "    #print(\"width of box {:.2f}\".format(width))\n",
        "    #print(\"height of box {:.2f}\".format(height))\n",
        "    #print(\"x_min {:.2f}\".format(x_min))\n",
        "    #print(\"y_min {:.2f}\".format(y_min))\n",
        "    #print(\"x_max {:.2f}\".format(y_max))\n",
        "    #print(\"y_max {:.2f}\".format(y_max))\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "images_dir = testingImage_folder_path\n",
        "annotations_dir = testingLabel_folder_path\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "testing_dataset = TestingDataset(images_dir, annotations_dir, transform)\n",
        "test1_dataloader = DataLoader(dataset, batch_size =4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iklYcx76OZz-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def plot_image_with_boxes(image, target, predictions=None, ax=None):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
        "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
        "    image = image * 255.0\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    for box in target['boxes']:\n",
        "        x_min, y_min, x_max, y_max = box.cpu().numpy()\n",
        "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "    if predictions:\n",
        "        for box in predictions['boxes']:\n",
        "            x_min, y_min, x_max, y_max = box.cpu().detach().numpy()\n",
        "            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='b', facecolor='none', linestyle='dashed')\n",
        "            ax.add_patch(rect)\n",
        "    ax.axis('off')\n",
        "\n",
        "def visualize_batch(images, targets, predictions=None, index=0):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
        "    img = images[index]\n",
        "    tgt = targets[index]\n",
        "    pred = predictions[index] if predictions else None\n",
        "    plot_image_with_boxes(img, tgt, pred, ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for batch in dataloader:\n",
        "    for img, tgt in zip(batch[0], batch[1]):\n",
        "        print(tgt)\n",
        "    visualize_batch(batch[0], batch[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zFBrp_e_r0m"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.80 * len(dataset))\n",
        "dev_size = int(0.20 * len(dataset))\n",
        "test_size = int(0.61 * len(testing_dataset))\n",
        "\n",
        "extra_size = len(dataset) - dev_size - train_size\n",
        "extra_testing_size = len(testing_dataset) - test_size\n",
        "\n",
        "print(train_size)\n",
        "print(dev_size)\n",
        "print(test_size)\n",
        "\n",
        "train_dataset, dev_dataset, extra_dataset = random_split(dataset, [train_size, dev_size, extra_size])\n",
        "test_dataset, extra_dataset = random_split(testing_dataset, [test_size, extra_testing_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn) # training set\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn) # validation set\n",
        "test2_dataloader = DataLoader(testing_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn) # validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YevhYgheVPt2"
      },
      "outputs": [],
      "source": [
        "num_classes = 19\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.roi_heads.score_thresh = 0.0000005\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum = 0.9, weight_decay=0.0005)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "num_epochs = 4\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if batch[0] is None or batch[1] is None:\n",
        "            continue\n",
        "\n",
        "        images, targets = batch\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        if any(target['boxes'].numel() == 0 for target in targets):\n",
        "                continue  # Skip batches with no boxes\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ground_truth_labels = [target['labels'].to(device) for target in targets]\n",
        "        ground_truth_boxes = [target['boxes'].to(device) for target in targets]\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          predictions = model(images)\n",
        "        model.train()\n",
        "\n",
        "        for prediction, ground_truth in zip(predictions, ground_truth_labels):\n",
        "            predicted_labels = prediction['labels']\n",
        "            total_train += len(predicted_labels)\n",
        "            matched = sum(1 for pl, gl in zip(predicted_labels, ground_truth) if pl == gl)\n",
        "            correct_train += matched\n",
        "\n",
        "            # Print predicted and ground truth labels for visual comparison\n",
        "            #for pl, gl in zip(predicted_labels, ground_truth):\n",
        "             #   print(f'Epoch [{epochs+1}/{num_epochs}], Step [{step+1}/{len(train_dataloader)}]')\n",
        "              #  print(f'Predicted: {pl.item()}, Ground Truth: {gl.item()}')\n",
        "\n",
        "        #print(predictions)\n",
        "        #print(ground_truth_labels)\n",
        "        #print(ground_truth_boxes)\n",
        "        visualize_batch(images, targets, predictions)\n",
        "\n",
        "        print(f'Loss: {losses.item()}')\n",
        "\n",
        "        if step % 50 == 0:\n",
        "          print(f'Epoch [{epochs+1}/{num_epochs}], Step [{step+1}/{len(train_dataloader)}], Training Loss: {losses.item():.4f}')\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    #Validation Loop\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for batch in dev_dataloader:\n",
        "        if batch[0] is None or batch[1] is None:\n",
        "            continue\n",
        "\n",
        "        images, targets = batch\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "        if any(target['boxes'].numel() == 0 for target in targets):\n",
        "                continue  # Skip batches with no boxes\n",
        "\n",
        "        ground_truth_labels = [target['labels'].to(device) for target in targets]\n",
        "        predictions = model(images)\n",
        "\n",
        "        for prediction, ground_truth in zip(predictions, ground_truth_labels):\n",
        "                predicted_labels = prediction['labels']\n",
        "                matched = sum(1 for pl, gl in zip(predicted_labels, ground_truth) if pl == gl)\n",
        "                total_val += len(predicted_labels)\n",
        "                correct_val += matched\n",
        "\n",
        "                for pl, gl in zip(predicted_labels, ground_truth):\n",
        "                  print(f'Predicted: {pl.item()}, Ground Truth: {gl.item()}')\n",
        "\n",
        "        model.train()\n",
        "        loss_dict = model(images, targets)\n",
        "        model.eval()\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        val_loss += losses\n",
        "        print(f'Loss: {losses.item()}')\n",
        "\n",
        "    val_accuracy = correct_val / total_val if total_val > 0 else 0\n",
        "    val_loss /= len(dev_dataloader)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epochs+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNnwzpNQuXDF"
      },
      "outputs": [],
      "source": [
        "test_accuracies = []\n",
        "#Testing Loop\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "  for batch in test2_dataloader:\n",
        "    if batch[0] is None or batch[1] is None:\n",
        "        continue\n",
        "\n",
        "    images, targets = batch\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "    if any(target['boxes'].numel() == 0 for target in targets):\n",
        "            continue  # Skip batches with no boxes\n",
        "\n",
        "    ground_truth_labels = [target['labels'].to(device) for target in targets]\n",
        "    predictions = model(images)\n",
        "\n",
        "    for prediction, ground_truth in zip(predictions, ground_truth_labels):\n",
        "            predicted_labels = prediction['labels']\n",
        "            matched = sum(1 for pl, gl in zip(predicted_labels, ground_truth) if pl == gl)\n",
        "            total_test += len(predicted_labels)\n",
        "            correct_test += matched\n",
        "\n",
        "            for pl, gl in zip(predicted_labels, ground_truth):\n",
        "              print(f'Predicted: {pl.item()}, Ground Truth: {gl.item()}')\n",
        "\n",
        "    visualize_batch(images, targets, predictions)\n",
        "    model.train()\n",
        "    loss_dict = model(images, targets)\n",
        "    model.eval()\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "    test_loss += losses\n",
        "    print(f'Loss: {losses.item()}')\n",
        "\n",
        "test_accuracy = correct_test / total_test if total_test > 0 else 0\n",
        "test_loss /= len(test2_dataloader)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "print(f'Epoch [{epochs+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNBcucmbY9pmndGVVbErzrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}